{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - Laboratory 2: Naive Bayes Classifiers\n",
    "## Santiago Álvarez Sepúlveda\n",
    "## e-mail: saalvarezse@unal.edu.co\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document's Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 u' chinese beijing chinese']\n",
      " [1 u' chinese chinese shanghai']\n",
      " [1 u' chinese macao']\n",
      " [0 u' tokyo japan chinese']\n",
      " [1 u' taipei taiwan']\n",
      " [1 u' macao taiwan shanghai']\n",
      " [0 u' japan sapporo']\n",
      " [0 u' sapporo osaka taiwan']\n",
      " [1 u' chinese chinese chinese tokyo japan']\n",
      " [0 u' taiwan taiwan sapporo']]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./documentsChina.csv', usecols=[0,1], encoding='latin-1')\n",
    "df.columns = ['Class','Document']\n",
    "# label spam as 1, not spam as 0\n",
    "df['Class'] = df['Class'].replace([\"yes\",\"no\"],[1,0])\n",
    "# Remove Upper case letters\n",
    "df['Document'] = df.Document.map(lambda x: x.lower())\n",
    "# Remove any puntuation\n",
    "df['Document'] = df.Document.str.replace('[^\\w\\s]', '')  \n",
    "data = df.values\n",
    "print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ngrams_bayes():\n",
    "    \n",
    "    def __init__(self, data, n=2, split=0.75):\n",
    "        \n",
    "        # split into training and testing data\n",
    "        self.train_data, self.test_data = train_test_split(data, train_size=split)\n",
    "        # convert into n grams\n",
    "        self.train_data = [[item[0], self.ngrams(n, item[1])] for item in self.train_data]\n",
    "        self.test_data = [[item[0], self.ngrams(n, item[1])] for item in self.test_data]\n",
    "        print 'Training %d-gram data:'%(n)\n",
    "        print self.train_data\n",
    "        print '\\nTesting %d-gram data:'%(n)\n",
    "        print self.test_data\n",
    "        \n",
    "        # count unique n grams in training data\n",
    "        flattened = [gram for message in self.train_data for gram in message[1]]\n",
    "        self.unique = len(set(flattened))\n",
    "        print '\\nVocabulary in training data'\n",
    "        print set(flattened)\n",
    "        print '\\nUnique %d-grams in training data'%(n)\n",
    "        print 'N = %d'%(self.unique)\n",
    "        \n",
    "        # init dicts\n",
    "        self.trainPositive = {}\n",
    "        self.trainNegative = {}\n",
    "        # counters\n",
    "        self.posGramCount = 0\n",
    "        self.negGramCount = 0\n",
    "        self.spamCount = 0\n",
    "        self.negSpamCount = 0\n",
    "        # priors\n",
    "        self.pA = 0\n",
    "        self.pNotA = 0\n",
    "        \n",
    "    def ngrams(self, n, text):\n",
    "        text = text.split(' ')\n",
    "        grams = []\n",
    "        for i in range(len(text)-n+1):\n",
    "            gram = ' '.join(text[i:i+n])\n",
    "            grams.append(gram)\n",
    "        return grams \n",
    "    \n",
    "    def train(self):\n",
    "        print '\\nTraining Naive Bayesian Classifier'\n",
    "        for item in self.train_data:\n",
    "            label = item[0]\n",
    "            grams = item[1]\n",
    "            print '\\n- Class (c): %d\\n- grams:'%(label)\n",
    "            print grams\n",
    "            if label == 1:\n",
    "                self.spamCount += 1\n",
    "            else:\n",
    "                self.negSpamCount += 1\n",
    "            for gram in grams:\n",
    "                if label == 1:\n",
    "                    self.trainPositive[gram] = self.trainPositive.get(gram, 0) + 1\n",
    "                    self.posGramCount += 1\n",
    "                else:\n",
    "                    self.trainNegative[gram] = self.trainNegative.get(gram, 0) + 1\n",
    "                    self.negGramCount += 1\n",
    "        print '\\nPositive Training Vocabulary'\n",
    "        print self.trainPositive\n",
    "        print '\\nNegative Training Vocabulary'\n",
    "        print self.trainNegative\n",
    "        self.pA = self.spamCount/float(len(self.train_data))\n",
    "        self.pNotA = 1.0 - self.pA\n",
    "        print '\\nPrior(1) = %f'%(self.pA)\n",
    "        print '\\nPrior(0) = %f'%(self.pNotA)\n",
    "        \n",
    "    def classify(self, text, alpha=1.0):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        isSpam = self.pA * self.conditionalText(text, 1)\n",
    "        notSpam = self.pNotA * self.conditionalText(text, 0)\n",
    "        print 'P(t|1) = %f'%(isSpam)\n",
    "        print 'P(t|0) = %f'%(notSpam)\n",
    "        if (isSpam > notSpam):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def conditionalText(self, grams, label):\n",
    "        result = 1.0\n",
    "        for ngram in grams:\n",
    "            result *= self.conditionalNgram(ngram, label)\n",
    "        return result\n",
    "    \n",
    "    def conditionalNgram(self, ngram, label):\n",
    "        alpha = self.alpha\n",
    "        if label == 1:\n",
    "            return ((self.trainPositive.get(ngram,0)+alpha) /\n",
    "                    float(self.posGramCount+alpha*self.unique))\n",
    "        else:\n",
    "            return ((self.trainNegative.get(ngram,0)+alpha) /\n",
    "                    float(self.negGramCount+alpha*self.unique))\n",
    "            \n",
    "    def evaluate_test_data(self):\n",
    "        print '\\nTesting Naive Bayesian Classifier'\n",
    "        results = []\n",
    "        for test in self.test_data:\n",
    "            label = test[0]\n",
    "            text = test[1]\n",
    "            print '\\n- Class (c): %d\\n- grams:'%(label)\n",
    "            print text\n",
    "            ruling = self.classify(text)\n",
    "            print '- Classified as (%d)\\n-labeled as (%d)'%(ruling, label)\n",
    "            if ruling == label:\n",
    "                print 'correct!'\n",
    "                results.append(1) \n",
    "            else:\n",
    "                print 'wrong.'\n",
    "                results.append(0) \n",
    "        print(\"Evaluated {} test cases. {:.2f}% Accuracy\".format(len(results), 100.0*sum(results)/float(len(results))))\n",
    "        return sum(results)/float(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1-gram data:\n",
      "[[0, [u'', u'taiwan', u'taiwan', u'sapporo']], [1, [u'', u'chinese', u'beijing', u'chinese']], [0, [u'', u'japan', u'sapporo']], [1, [u'', u'chinese', u'chinese', u'chinese', u'tokyo', u'japan']], [1, [u'', u'macao', u'taiwan', u'shanghai']], [0, [u'', u'tokyo', u'japan', u'chinese']], [1, [u'', u'chinese', u'macao']]]\n",
      "\n",
      "Testing 1-gram data:\n",
      "[[1, [u'', u'taipei', u'taiwan']], [0, [u'', u'sapporo', u'osaka', u'taiwan']], [1, [u'', u'chinese', u'chinese', u'shanghai']]]\n",
      "\n",
      "Vocabulary in training data\n",
      "set([u'', u'beijing', u'chinese', u'tokyo', u'shanghai', u'japan', u'taiwan', u'macao', u'sapporo'])\n",
      "\n",
      "Unique 1-grams in training data\n",
      "N = 9\n",
      "\n",
      "Training Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u'', u'taiwan', u'taiwan', u'sapporo']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'chinese', u'beijing', u'chinese']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u'', u'japan', u'sapporo']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'chinese', u'chinese', u'chinese', u'tokyo', u'japan']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'macao', u'taiwan', u'shanghai']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u'', u'tokyo', u'japan', u'chinese']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'chinese', u'macao']\n",
      "\n",
      "Positive Training Vocabulary\n",
      "{u'': 4, u'beijing': 1, u'chinese': 6, u'tokyo': 1, u'shanghai': 1, u'japan': 1, u'taiwan': 1, u'macao': 2}\n",
      "\n",
      "Negative Training Vocabulary\n",
      "{u'': 3, u'chinese': 1, u'tokyo': 1, u'japan': 2, u'taiwan': 2, u'sapporo': 2}\n",
      "\n",
      "Prior(1) = 0.571429\n",
      "\n",
      "Prior(0) = 0.428571\n",
      "\n",
      "Testing Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'taipei', u'taiwan']\n",
      "P(t|1) = 0.000325\n",
      "P(t|0) = 0.000643\n",
      "- Classified as (0)\n",
      "-labeled as (1)\n",
      "wrong.\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u'', u'sapporo', u'osaka', u'taiwan']\n",
      "P(t|1) = 0.000013\n",
      "P(t|0) = 0.000096\n",
      "- Classified as (0)\n",
      "-labeled as (0)\n",
      "correct!\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'chinese', u'chinese', u'shanghai']\n",
      "P(t|1) = 0.000613\n",
      "P(t|0) = 0.000043\n",
      "- Classified as (1)\n",
      "-labeled as (1)\n",
      "correct!\n",
      "Evaluated 3 test cases. 66.67% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bayes = ngrams_bayes(data,1)\n",
    "unigram_bayes.train()\n",
    "unigram_bayes.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2-gram data:\n",
      "[[1, [u' macao', u'macao taiwan', u'taiwan shanghai']], [1, [u' chinese', u'chinese chinese', u'chinese shanghai']], [0, [u' tokyo', u'tokyo japan', u'japan chinese']], [1, [u' chinese', u'chinese macao']], [0, [u' sapporo', u'sapporo osaka', u'osaka taiwan']], [1, [u' chinese', u'chinese chinese', u'chinese chinese', u'chinese tokyo', u'tokyo japan']], [0, [u' japan', u'japan sapporo']]]\n",
      "\n",
      "Testing 2-gram data:\n",
      "[[1, [u' taipei', u'taipei taiwan']], [1, [u' chinese', u'chinese beijing', u'beijing chinese']], [0, [u' taiwan', u'taiwan taiwan', u'taiwan sapporo']]]\n",
      "\n",
      "Vocabulary in training data\n",
      "set([u'macao taiwan', u' tokyo', u'tokyo japan', u'taiwan shanghai', u' chinese', u'chinese chinese', u'chinese tokyo', u'sapporo osaka', u'japan chinese', u' sapporo', u'osaka taiwan', u'chinese macao', u'chinese shanghai', u' japan', u'japan sapporo', u' macao'])\n",
      "\n",
      "Unique 2-grams in training data\n",
      "N = 16\n",
      "\n",
      "Training Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' macao', u'macao taiwan', u'taiwan shanghai']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese', u'chinese chinese', u'chinese shanghai']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' tokyo', u'tokyo japan', u'japan chinese']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese', u'chinese macao']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' sapporo', u'sapporo osaka', u'osaka taiwan']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese', u'chinese chinese', u'chinese chinese', u'chinese tokyo', u'tokyo japan']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' japan', u'japan sapporo']\n",
      "\n",
      "Positive Training Vocabulary\n",
      "{u'macao taiwan': 1, u'chinese macao': 1, u'taiwan shanghai': 1, u' chinese': 3, u'chinese chinese': 3, u'chinese tokyo': 1, u'tokyo japan': 1, u'chinese shanghai': 1, u' macao': 1}\n",
      "\n",
      "Negative Training Vocabulary\n",
      "{u' tokyo': 1, u'tokyo japan': 1, u'sapporo osaka': 1, u' sapporo': 1, u'osaka taiwan': 1, u'japan chinese': 1, u' japan': 1, u'japan sapporo': 1}\n",
      "\n",
      "Prior(1) = 0.571429\n",
      "\n",
      "Prior(0) = 0.428571\n",
      "\n",
      "Testing Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' taipei', u'taipei taiwan']\n",
      "P(t|1) = 0.000679\n",
      "P(t|0) = 0.000744\n",
      "- Classified as (0)\n",
      "-labeled as (1)\n",
      "wrong.\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese', u'chinese beijing', u'beijing chinese']\n",
      "P(t|1) = 0.000094\n",
      "P(t|0) = 0.000031\n",
      "- Classified as (1)\n",
      "-labeled as (1)\n",
      "correct!\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' taiwan', u'taiwan taiwan', u'taiwan sapporo']\n",
      "P(t|1) = 0.000023\n",
      "P(t|0) = 0.000031\n",
      "- Classified as (0)\n",
      "-labeled as (0)\n",
      "correct!\n",
      "Evaluated 3 test cases. 66.67% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_sms= ngrams_bayes(data,2) \n",
    "bigram_sms.train()\n",
    "bigram_sms.evaluate_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3-gram data:\n",
      "[[1, [u' chinese chinese', u'chinese chinese shanghai']], [1, [u' chinese beijing', u'chinese beijing chinese']], [0, [u' tokyo japan', u'tokyo japan chinese']], [0, [u' japan sapporo']], [1, [u' macao taiwan', u'macao taiwan shanghai']], [0, [u' sapporo osaka', u'sapporo osaka taiwan']], [0, [u' taiwan taiwan', u'taiwan taiwan sapporo']]]\n",
      "\n",
      "Testing 3-gram data:\n",
      "[[1, [u' chinese chinese', u'chinese chinese chinese', u'chinese chinese tokyo', u'chinese tokyo japan']], [1, [u' taipei taiwan']], [1, [u' chinese macao']]]\n",
      "\n",
      "Vocabulary in training data\n",
      "set([u' tokyo japan', u' chinese beijing', u'chinese chinese shanghai', u'tokyo japan chinese', u'sapporo osaka taiwan', u' macao taiwan', u'macao taiwan shanghai', u' chinese chinese', u'chinese beijing chinese', u' taiwan taiwan', u' japan sapporo', u'taiwan taiwan sapporo', u' sapporo osaka'])\n",
      "\n",
      "Unique 3-grams in training data\n",
      "N = 13\n",
      "\n",
      "Training Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese chinese', u'chinese chinese shanghai']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese beijing', u'chinese beijing chinese']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' tokyo japan', u'tokyo japan chinese']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' japan sapporo']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' macao taiwan', u'macao taiwan shanghai']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' sapporo osaka', u'sapporo osaka taiwan']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' taiwan taiwan', u'taiwan taiwan sapporo']\n",
      "\n",
      "Positive Training Vocabulary\n",
      "{u' macao taiwan': 1, u' chinese beijing': 1, u'chinese chinese shanghai': 1, u'macao taiwan shanghai': 1, u' chinese chinese': 1, u'chinese beijing chinese': 1}\n",
      "\n",
      "Negative Training Vocabulary\n",
      "{u' tokyo japan': 1, u'sapporo osaka taiwan': 1, u'tokyo japan chinese': 1, u'taiwan taiwan sapporo': 1, u' sapporo osaka': 1, u' taiwan taiwan': 1, u' japan sapporo': 1}\n",
      "\n",
      "Prior(1) = 0.428571\n",
      "\n",
      "Prior(0) = 0.571429\n",
      "\n",
      "Testing Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese chinese', u'chinese chinese chinese', u'chinese chinese tokyo', u'chinese tokyo japan']\n",
      "P(t|1) = 0.000007\n",
      "P(t|0) = 0.000004\n",
      "- Classified as (1)\n",
      "-labeled as (1)\n",
      "correct!\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' taipei taiwan']\n",
      "P(t|1) = 0.022556\n",
      "P(t|0) = 0.028571\n",
      "- Classified as (0)\n",
      "-labeled as (1)\n",
      "wrong.\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese macao']\n",
      "P(t|1) = 0.022556\n",
      "P(t|0) = 0.028571\n",
      "- Classified as (0)\n",
      "-labeled as (1)\n",
      "wrong.\n",
      "Evaluated 3 test cases. 33.33% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sms = ngrams_bayes(data,3) \n",
    "trigram_sms.train()\n",
    "trigram_sms.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our bayesian classifier performs well with unigrams, ok with bigrams, and is basically guessing randomly when it comes to trigrams. Increasing the size of your grams does not help this classifier classify this dataset. This is likely due to the data being made of up of short messages with highly specific and colloquial words. Nearly none of the trigrams will occur more than once in this dataset. I imagine that larger ngrams used in a baysian classifer would work well with something like product reviews which are longer than text messages and use less colloquial language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classifier with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Preprocessing of the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class                                   Document\n",
      "0      1                [chinese, beijing, chinese]\n",
      "1      1               [chinese, chinese, shanghai]\n",
      "2      1                           [chinese, macao]\n",
      "3      0                    [tokyo, japan, chinese]\n",
      "4      1                           [taipei, taiwan]\n",
      "5      1                  [macao, taiwan, shanghai]\n",
      "6      0                           [japan, sapporo]\n",
      "7      0                   [sapporo, osaka, taiwan]\n",
      "8      1  [chinese, chinese, chinese, tokyo, japan]\n",
      "9      0                  [taiwan, taiwan, sapporo]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words in every message\n",
    "df['Document'] = df['Document'].apply(nltk.word_tokenize)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class                                Document\n",
      "0      1                  [chines, beij, chines]\n",
      "1      1              [chines, chines, shanghai]\n",
      "2      1                         [chines, macao]\n",
      "3      0                  [tokyo, japan, chines]\n",
      "4      1                        [taipei, taiwan]\n",
      "5      1               [macao, taiwan, shanghai]\n",
      "6      0                        [japan, sapporo]\n",
      "7      0                [sapporo, osaka, taiwan]\n",
      "8      1  [chines, chines, chines, tokyo, japan]\n",
      "9      0               [taiwan, taiwan, sapporo]\n"
     ]
    }
   ],
   "source": [
    "# Use Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "df['Document'] = df['Document'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (2, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 9)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 7)\t1\n",
      "  (5, 8)\t1\n",
      "  (5, 3)\t1\n",
      "  (5, 6)\t1\n",
      "  (6, 5)\t1\n",
      "  (6, 2)\t1\n",
      "  (7, 4)\t1\n",
      "  (7, 5)\t1\n",
      "  (7, 8)\t1\n",
      "  (8, 2)\t1\n",
      "  (8, 9)\t1\n",
      "  (8, 1)\t3\n",
      "  (9, 5)\t1\n",
      "  (9, 8)\t2\n",
      "   Class                          Document\n",
      "0      1                chines beij chines\n",
      "1      1            chines chines shanghai\n",
      "2      1                      chines macao\n",
      "3      0                tokyo japan chines\n",
      "4      1                     taipei taiwan\n",
      "5      1             macao taiwan shanghai\n",
      "6      0                     japan sapporo\n",
      "7      0              sapporo osaka taiwan\n",
      "8      1  chines chines chines tokyo japan\n",
      "9      0             taiwan taiwan sapporo\n"
     ]
    }
   ],
   "source": [
    "# This converts the list of words into space-separated strings\n",
    "df['Document'] = df['Document'].apply(lambda x: ' '.join(x))\n",
    "count_vect = CountVectorizer()  \n",
    "counts = count_vect.fit_transform(df['Document'])\n",
    "print counts\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.764949063238371\n",
      "  (0, 0)\t0.6440907782686681\n",
      "  (1, 1)\t0.8131579832939391\n",
      "  (1, 6)\t0.5820430346678277\n",
      "  (2, 1)\t0.5726575030814076\n",
      "  (2, 3)\t0.8197947207469487\n",
      "  (3, 1)\t0.4653431056969696\n",
      "  (3, 9)\t0.6661675073383854\n",
      "  (3, 2)\t0.5828178498869621\n",
      "  (4, 7)\t0.8341378713086336\n",
      "  (4, 8)\t0.5515559913995145\n",
      "  (5, 6)\t0.61957540186839\n",
      "  (5, 3)\t0.61957540186839\n",
      "  (5, 8)\t0.48192597232276857\n",
      "  (6, 2)\t0.7071067811865475\n",
      "  (6, 5)\t0.7071067811865475\n",
      "  (7, 8)\t0.4686906257315891\n",
      "  (7, 5)\t0.5271685592911145\n",
      "  (7, 4)\t0.708817612257386\n",
      "  (8, 1)\t0.8445513958589379\n",
      "  (8, 9)\t0.4030092286105375\n",
      "  (8, 2)\t0.352585452631641\n",
      "  (9, 8)\t0.8716186013638371\n",
      "  (9, 5)\t0.4901846731146827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anatiago1/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)  \n",
    "print counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "  (0, 8)\t0.48192597232276857\n",
      "  (0, 3)\t0.61957540186839\n",
      "  (0, 6)\t0.61957540186839\n",
      "  (1, 0)\t0.6440907782686681\n",
      "  (1, 1)\t0.764949063238371\n",
      "  (2, 5)\t0.4901846731146827\n",
      "  (2, 8)\t0.8716186013638371\n",
      "  (3, 6)\t0.5820430346678277\n",
      "  (3, 1)\t0.8131579832939391\n",
      "  (4, 8)\t0.5515559913995145\n",
      "  (4, 7)\t0.8341378713086336\n",
      "  (5, 4)\t0.708817612257386\n",
      "  (5, 5)\t0.5271685592911145\n",
      "  (5, 8)\t0.4686906257315891\n",
      "  (6, 5)\t0.7071067811865475\n",
      "  (6, 2)\t0.7071067811865475\n",
      "\n",
      "X_test:\n",
      "  (0, 2)\t0.5828178498869621\n",
      "  (0, 9)\t0.6661675073383854\n",
      "  (0, 1)\t0.4653431056969696\n",
      "  (1, 2)\t0.352585452631641\n",
      "  (1, 9)\t0.4030092286105375\n",
      "  (1, 1)\t0.8445513958589379\n",
      "  (2, 3)\t0.8197947207469487\n",
      "  (2, 1)\t0.5726575030814076\n",
      "\n",
      "y_train:\n",
      "5    1\n",
      "0    1\n",
      "9    0\n",
      "1    1\n",
      "4    1\n",
      "7    0\n",
      "6    0\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "y_test:\n",
      "3    0\n",
      "8    1\n",
      "2    1\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Accuracy: 0.666667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[0 1]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(counts, df['Class'], test_size=0.25, random_state=69)\n",
    "print 'X_train:'\n",
    "print X_train\n",
    "print '\\nX_test:'\n",
    "print X_test\n",
    "print '\\ny_train:'\n",
    "print y_train\n",
    "print '\\ny_test:'\n",
    "print y_test\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "print '\\nAccuracy: %f'%(np.mean(predicted == y_test))\n",
    "print '\\nConfusion Matrix:'\n",
    "print confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point it has to be said that the results of the classifiers implemented by hand, using python basic tools, and using scikitlearn have thrown differnt results, mainly because of the information they are mannaging in each case. \n",
    "\n",
    "First of all, actually the classifier implemented by hand and using basic python tools work exactly the same, but they differ int he final result because in the python script, symbols like spaces (\" \") are being considered as tokens, this changes the weights of every word in each document and in fact the probabilities of a word to belong to a document of a certain class, if this symbols were not taken in count, the results should be exactly the same.\n",
    "\n",
    "On the other hand, the classifier of scikitlearn is giving some differences in the results at a probabilities level, not so much at the decission level, because it changes the information given to the classifier, and it has generated some tokens for each document, or it has coded the documents to handle their information in an easier way, this affects the numerical results of the classifier, but not actually the decisions it takes.\n",
    "\n",
    "All of this methods are different aproaches to the Naive Bayesian Classifiers, which can throw different results depending on the amount of information it has to handle and how the information is represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training and Evaluating the Multivariate Bernoulli Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "  (0, 8)\t0.48192597232276857\n",
      "  (0, 3)\t0.61957540186839\n",
      "  (0, 6)\t0.61957540186839\n",
      "  (1, 0)\t0.6440907782686681\n",
      "  (1, 1)\t0.764949063238371\n",
      "  (2, 5)\t0.4901846731146827\n",
      "  (2, 8)\t0.8716186013638371\n",
      "  (3, 6)\t0.5820430346678277\n",
      "  (3, 1)\t0.8131579832939391\n",
      "  (4, 8)\t0.5515559913995145\n",
      "  (4, 7)\t0.8341378713086336\n",
      "  (5, 4)\t0.708817612257386\n",
      "  (5, 5)\t0.5271685592911145\n",
      "  (5, 8)\t0.4686906257315891\n",
      "  (6, 5)\t0.7071067811865475\n",
      "  (6, 2)\t0.7071067811865475\n",
      "\n",
      "X_test:\n",
      "  (0, 2)\t0.5828178498869621\n",
      "  (0, 9)\t0.6661675073383854\n",
      "  (0, 1)\t0.4653431056969696\n",
      "  (1, 2)\t0.352585452631641\n",
      "  (1, 9)\t0.4030092286105375\n",
      "  (1, 1)\t0.8445513958589379\n",
      "  (2, 3)\t0.8197947207469487\n",
      "  (2, 1)\t0.5726575030814076\n",
      "\n",
      "y_train:\n",
      "5    1\n",
      "0    1\n",
      "9    0\n",
      "1    1\n",
      "4    1\n",
      "7    0\n",
      "6    0\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "y_test:\n",
      "3    0\n",
      "8    1\n",
      "2    1\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Accuracy: 0.666667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[0 1]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(counts, df['Class'], test_size=0.25, random_state=69)\n",
    "print 'X_train:'\n",
    "print X_train\n",
    "print '\\nX_test:'\n",
    "print X_test\n",
    "print '\\ny_train:'\n",
    "print y_train\n",
    "print '\\ny_test:'\n",
    "print y_test\n",
    "model = BernoulliNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "print '\\nAccuracy: %f'%(np.mean(predicted == y_test))\n",
    "print '\\nConfusion Matrix:'\n",
    "print confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the Multinomial NB classifier and the Bernoulli NB classifier, we see that they don't differ much in the decissions they make and the accuracy they have to do the job correctly. They are different approaches inside, as they have different models to calculate the conditional probabilities and the escence of the classification (binary to counting) changes a bit the information handeled but not the way the process happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive Bayesian Classification System for lines from Biggie Smalls and 2Pac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biggie_df = pd.read_csv('./biggie_lyrics.csv', usecols=[1], encoding='latin-1', header=None)\n",
    "biggie_df.columns = [\"lyrics\"]\n",
    "biggie_df[\"lyrics\"] = biggie_df[\"lyrics\"].str.replace('[^\\w\\s]','')\n",
    "biggie_df[\"lyrics\"] = biggie_df[\"lyrics\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relax and take notes while i take tokes of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good evenin ladies and gentlemen\\nhows everybo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>who shot ya\\nseperate the weak from the obsole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>when i die fuck it i wanna go to hell\\ncause i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>when the lala hits ya lyrics just splits ya\\nh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lyrics\n",
       "11  relax and take notes while i take tokes of the...\n",
       "12  good evenin ladies and gentlemen\\nhows everybo...\n",
       "13  who shot ya\\nseperate the weak from the obsole...\n",
       "14  when i die fuck it i wanna go to hell\\ncause i...\n",
       "15  when the lala hits ya lyrics just splits ya\\nh..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggie_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pac_df = pd.read_csv('./2pac_lyrics.csv', usecols=[1], encoding='latin-1', header=None)\n",
    "pac_df.columns = [\"lyrics\"]\n",
    "pac_df[\"lyrics\"] = pac_df[\"lyrics\"].str.replace('[^\\w\\s]','')\n",
    "pac_df[\"lyrics\"] = pac_df[\"lyrics\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little something for my godson elijah\\nand a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yo mo bee mayn drop that shit\\nyou know what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rest in peace to my motherfucker biggy smallz\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>makaveli in this killuminati\\nall through your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its just me against the world\\nnothin to lose\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics\n",
       "0  little something for my godson elijah\\nand a l...\n",
       "1  yo mo bee mayn drop that shit\\nyou know what t...\n",
       "2  rest in peace to my motherfucker biggy smallz\\...\n",
       "3  makaveli in this killuminati\\nall through your...\n",
       "4  its just me against the world\\nnothin to lose\\..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fuck all you hoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>get a grip motherfucker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>yeah this album is dedicated to all the teache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>id never amount to nothin to all the people th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>buildings that i was hustlin in front of that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               line\n",
       "0      0                                  fuck all you hoes\n",
       "1      0                            get a grip motherfucker\n",
       "2      0  yeah this album is dedicated to all the teache...\n",
       "3      0  id never amount to nothin to all the people th...\n",
       "4      0  buildings that i was hustlin in front of that ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggie_lyrics = biggie_df[\"lyrics\"].values\n",
    "biggie_lyrics = [ song.split('\\n') for song in biggie_lyrics]\n",
    "biggie_lyrics = [line for song in biggie_lyrics for line in song]\n",
    "pac_lyrics = pac_df[\"lyrics\"].values\n",
    "pac_lyrics = [ song.split('\\n') for song in pac_lyrics]\n",
    "pac_lyrics = [line for song in pac_lyrics for line in song]\n",
    "\n",
    "rap_lines = [] \n",
    "\n",
    "for line in biggie_lyrics:\n",
    "    if len(line.split()) > 3:\n",
    "        rap_lines.append(np.array([0,str(line)]))\n",
    "        \n",
    "for line in pac_lyrics:\n",
    "    if len(line.split()) > 3:\n",
    "        rap_lines.append(np.array([1,str(line)]))\n",
    "        \n",
    "rap_lines = np.array(rap_lines)\n",
    "rap_lines = pd.DataFrame(rap_lines)\n",
    "rap_lines.columns = [\"label\",\"line\"]\n",
    "rap_lines['label'] = rap_lines['label'].replace(['0','1'],[0,1])\n",
    "rap_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 849)\t0.49087318817854825\n",
      "  (0, 58)\t0.43268940172940906\n",
      "  (0, 2501)\t0.31850678004835703\n",
      "  (0, 1027)\t0.6858402334521511\n",
      "  (1, 877)\t0.41830230655719114\n",
      "  (1, 932)\t0.7601505859197147\n",
      "  (1, 1431)\t0.49718635043095055\n",
      "  (2, 58)\t0.20803213246469884\n",
      "  (2, 2493)\t0.34411189665671293\n",
      "  (2, 2215)\t0.2250169996870463\n",
      "  (2, 53)\t0.4322487816477064\n",
      "  (2, 1131)\t0.2057966169419576\n",
      "  (2, 561)\t0.410044488257387\n",
      "  (2, 2255)\t0.1425118077471302\n",
      "  (2, 2203)\t0.1160197544126776\n",
      "  (2, 2176)\t0.4322487816477064\n",
      "  (2, 2201)\t0.2028127062967269\n",
      "  (2, 2261)\t0.3183733108460118\n",
      "  (2, 1357)\t0.16506204370082317\n",
      "  (3, 58)\t0.20116206656478056\n",
      "  (3, 2255)\t0.2756109781373328\n",
      "  (3, 2203)\t0.22437662185626947\n",
      "  (3, 2201)\t0.19611500704665685\n",
      "  (3, 1089)\t0.3383271897309154\n",
      "  (3, 1467)\t0.24968336690403506\n",
      "  :\t:\n",
      "  (1967, 2203)\t0.15468580785408723\n",
      "  (1967, 1357)\t0.22007248425203085\n",
      "  (1967, 1515)\t0.2704043589421255\n",
      "  (1967, 1239)\t0.34580186212426445\n",
      "  (1967, 2102)\t0.37858120072806645\n",
      "  (1967, 1199)\t0.5094034783786443\n",
      "  (1967, 1893)\t0.5763048915384901\n",
      "  (1968, 2255)\t0.1421446866227497\n",
      "  (1968, 2203)\t0.11572087880816694\n",
      "  (1968, 78)\t0.15708416007245224\n",
      "  (1968, 118)\t0.2635537662293054\n",
      "  (1968, 633)\t0.25213286835978227\n",
      "  (1968, 846)\t0.2553156008029875\n",
      "  (1968, 1511)\t0.4311352763091721\n",
      "  (1968, 1832)\t0.4311352763091721\n",
      "  (1968, 143)\t0.4311352763091721\n",
      "  (1968, 92)\t0.4311352763091721\n",
      "  (1969, 1131)\t0.2319798097665941\n",
      "  (1969, 633)\t0.2849453932277956\n",
      "  (1969, 2211)\t0.24215750156044613\n",
      "  (1969, 1702)\t0.36286342155575707\n",
      "  (1969, 2428)\t0.3516087338941551\n",
      "  (1969, 1313)\t0.4444552497821418\n",
      "  (1969, 2205)\t0.43068062533342516\n",
      "  (1969, 321)\t0.4099102348400397\n",
      "      label                                               line\n",
      "0         0                                   fuck all you hoe\n",
      "1         0                              get a grip motherfuck\n",
      "2         0  yeah thi album is dedic to all the teacher tha...\n",
      "3         0  id never amount to nothin to all the peopl tha...\n",
      "4         0  build that i wa hustlin in front of that call ...\n",
      "5         0  me when i wa just tryin to make some money to ...\n",
      "6         0  and all the nigga in the struggl you know what...\n",
      "7         0                     uhha it all good babi baybe uh\n",
      "8         0                                  it wa all a dream\n",
      "9         0                      i use to read word up magazin\n",
      "10        0           saltnpepa and heavi d up in the limousin\n",
      "11        0                           hangin pictur on my wall\n",
      "12        0     everi saturday rap attack mr magic marley marl\n",
      "13        0                 i let my tape rock til my tape pop\n",
      "14        0      smokin weed and bamboo sippin on privat stock\n",
      "15        0   way back when i had the red and black lumberjack\n",
      "16        0                              with the hat to match\n",
      "17        0                     rememb rappin duke duhha duhha\n",
      "18        0  you never thought that hip hop would take it t...\n",
      "19        0         now im in the limelight caus i rhyme tight\n",
      "20        0      time to get paid blow up like the world trade\n",
      "21        0                born sinner the opposit of a winner\n",
      "22        0         rememb when i use to eat sardin for dinner\n",
      "23        0                   peac to ron g brucey b kid capri\n",
      "24        0                      funkmast flex lovebug starski\n",
      "25        0              im blowin up like you thought i would\n",
      "26        0                call the crib same number same hood\n",
      "27        0      uh and if you dont know now you know nigga uh\n",
      "28        0                     you know veri well who you are\n",
      "29        0       dont let em hold you down reach for the star\n",
      "...     ...                                                ...\n",
      "1940      1                dont let em jack you up back you up\n",
      "1941      1                  crack you up and pimpsmack you up\n",
      "1942      1                  you got ta learn to hold your own\n",
      "1943      1  they get jealou when they see you with your mo...\n",
      "1944      1               but tell the cop they cant touch thi\n",
      "1945      1  i dont trust thi when they tri to rush i bust thi\n",
      "1946      1                          that the sound of my tool\n",
      "1947      1    you say it aint cool my mama didnt rais no fool\n",
      "1948      1    and as long as i stay black i got ta stay strap\n",
      "1949      1                        and i never get to lay back\n",
      "1950      1         caus i alway got to worri bout the payback\n",
      "1951      1                 some buck that i rough up way back\n",
      "1952      1                     come back after all these year\n",
      "1953      1                ratatattattattat that the way it is\n",
      "1954      1     out on bail fresh out of jail california dream\n",
      "1955      1  soon as i step on the scene im hear hoochi scream\n",
      "1956      1                        fiend for money and alcohol\n",
      "1957      1  the life of a westsid player where coward die ...\n",
      "1958      1  onli in cali where we riot not ralli to live a...\n",
      "1959      1      in la we wear chuck not balli yeah that right\n",
      "1960      1  dress in loc and khaki suit and ride is what w...\n",
      "1961      1   floss but have caution we collid with other crew\n",
      "1962      1                            famou becaus we program\n",
      "1963      1  worldwid let them recogn from long beach to ro...\n",
      "1964      1          bump and grind like a slow jam it westsid\n",
      "1965      1        so you know the row wont bow down to no man\n",
      "1966      1  say what you say but give me that bomb beat fr...\n",
      "1967      1                    let me serenad the street of la\n",
      "1968      1  from oakland to sactown the bay area and back ...\n",
      "1969      1             cali is where they put their mack down\n",
      "\n",
      "[1970 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words in every message\n",
    "rap_lines['line'] = rap_lines['line'].apply(nltk.word_tokenize)\n",
    "# Use Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "rap_lines['line'] = rap_lines['line'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# This converts the list of words into space-separated strings\n",
    "rap_lines['line'] = rap_lines['line'].apply(lambda x: ' '.join(x))\n",
    "count_vect = CountVectorizer()  \n",
    "counts = count_vect.fit_transform(rap_lines['line'])\n",
    "#\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)  \n",
    "print counts\n",
    "print rap_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MultiNB_Sharokhian(counts, rap_lines, test_size=0.25, random_state=69):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(counts, rap_lines['label'], test_size=0.25, random_state=69)\n",
    "    model = MultinomialNB().fit(X_train, y_train)\n",
    "    predicted = model.predict(X_test)\n",
    "    accuracy = np.mean(predicted == y_test)\n",
    "    print '\\nAccuracy: %f'%(accuracy)\n",
    "    print '\\nConfusion Matrix:'\n",
    "    print confusion_matrix(y_test, predicted)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.691683569979716"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_MultiNB_Sharokhian(counts, rap_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n",
      "Average Accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for _ in range(10):\n",
    "    accuracy = train_MultiNB_Sharokhian(counts, rap_lines)\n",
    "    results.append(accuracy)\n",
    "print(\"Average Accuracy: {:.2f}\".format(sum(results)/float(len(results))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this method of classification for the data of N. Lidel, we see that the average result is pretty much the same as the one that Lidel obtained after doing the classification with his own method. So that shows that the Bayesian Classifiers All have very similar behaviours if the information the mannage is equal, and may differ if the information is changed or understood in a different way, mainly this is the greatest sensitivity that this classifiers have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
