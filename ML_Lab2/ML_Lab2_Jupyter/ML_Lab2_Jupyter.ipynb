{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, Bigrams, and Trigrams in Naive Bayes Classifiers\n",
    "\n",
    "Math of Intelligence Week 6 Challenge - https://www.youtube.com/watch?v=PrkiRVcrxOs&t=7s\n",
    "\n",
    "In this notebook I will explore the performance of ngram words in a naive bayes classifier. I will look at how they perform across two data sets: \n",
    "    1) A Spam SMS dataset \n",
    "    2) Rap lines from Biggie Smalls and 2Pac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document's Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 u' chinese beijing chinese']\n",
      " [1 u' chinese chinese shanghai']\n",
      " [1 u' chinese macao']\n",
      " [0 u' tokyo japan chinese']\n",
      " [1 u' taipei taiwan']\n",
      " [1 u' macao taiwan shanghai']\n",
      " [0 u' japan sapporo']\n",
      " [0 u' sapporo osaka taiwan']\n",
      " [1 u' chinese chinese chinese tokyo japan']\n",
      " [0 u' taiwan taiwan sapporo']]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./documentsChina.csv', usecols=[0,1], encoding='latin-1')\n",
    "df.columns = ['Class','Document']\n",
    "# label spam as 1, not spam as 0\n",
    "df['Class'] = df['Class'].replace([\"yes\",\"no\"],[1,0])\n",
    "# Remove Upper case letters\n",
    "df['Document'] = df.Document.map(lambda x: x.lower())\n",
    "# Remove any puntuation\n",
    "df['Document'] = df.Document.str.replace('[^\\w\\s]', '')  \n",
    "data = df.values\n",
    "print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ngrams_bayes():\n",
    "    \n",
    "    def __init__(self, data, n=2, split=0.75):\n",
    "        \n",
    "        # split into training and testing data\n",
    "        self.train_data, self.test_data = train_test_split(data, train_size=split)\n",
    "        # convert into n grams\n",
    "        self.train_data = [[item[0], self.ngrams(n, item[1])] for item in self.train_data]\n",
    "        self.test_data = [[item[0], self.ngrams(n, item[1])] for item in self.test_data]\n",
    "        print 'Training %d-gram data:'%(n)\n",
    "        print self.train_data\n",
    "        print '\\nTesting %d-gram data:'%(n)\n",
    "        print self.test_data\n",
    "        \n",
    "        # count unique n grams in training data\n",
    "        flattened = [gram for message in self.train_data for gram in message[1]]\n",
    "        self.unique = len(set(flattened))\n",
    "        print '\\nVocabulary in training data'\n",
    "        print set(flattened)\n",
    "        print '\\nUnique %d-grams in training data'%(n)\n",
    "        print 'N = %d'%(self.unique)\n",
    "        \n",
    "        # init dicts\n",
    "        self.trainPositive = {}\n",
    "        self.trainNegative = {}\n",
    "        # counters\n",
    "        self.posGramCount = 0\n",
    "        self.negGramCount = 0\n",
    "        self.spamCount = 0\n",
    "        self.negSpamCount = 0\n",
    "        # priors\n",
    "        self.pA = 0\n",
    "        self.pNotA = 0\n",
    "        \n",
    "    def ngrams(self, n, text):\n",
    "        text = text.split(' ')\n",
    "        grams = []\n",
    "        for i in range(len(text)-n+1):\n",
    "            gram = ' '.join(text[i:i+n])\n",
    "            grams.append(gram)\n",
    "        return grams \n",
    "    \n",
    "    def train(self):\n",
    "        print '\\nTraining Naive Bayesian Classifier'\n",
    "        for item in self.train_data:\n",
    "            label = item[0]\n",
    "            grams = item[1]\n",
    "            print '\\n- Class (c): %d\\n- grams:'%(label)\n",
    "            print grams\n",
    "            if label == 1:\n",
    "                self.spamCount += 1\n",
    "            else:\n",
    "                self.negSpamCount += 1\n",
    "            for gram in grams:\n",
    "                if label == 1:\n",
    "                    self.trainPositive[gram] = self.trainPositive.get(gram, 0) + 1\n",
    "                    self.posGramCount += 1\n",
    "                else:\n",
    "                    self.trainNegative[gram] = self.trainNegative.get(gram, 0) + 1\n",
    "                    self.negGramCount += 1\n",
    "        print '\\nPositive Training Vocabulary'\n",
    "        print self.trainPositive\n",
    "        print '\\nNegative Training Vocabulary'\n",
    "        print self.trainNegative\n",
    "        self.pA = self.spamCount/float(len(self.train_data))\n",
    "        self.pNotA = 1.0 - self.pA\n",
    "        print '\\nPrior(1) = %f'%(self.pA)\n",
    "        print '\\nPrior(0) = %f'%(self.pNotA)\n",
    "        \n",
    "    def classify(self, text, alpha=1.0):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        isSpam = self.pA * self.conditionalText(text, 1)\n",
    "        notSpam = self.pNotA * self.conditionalText(text, 0)\n",
    "        print 'P(t|1) = %f'%(isSpam)\n",
    "        print 'P(t|0) = %f'%(notSpam)\n",
    "        if (isSpam > notSpam):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def conditionalText(self, grams, label):\n",
    "        result = 1.0\n",
    "        for ngram in grams:\n",
    "            result *= self.conditionalNgram(ngram, label)\n",
    "        return result\n",
    "    \n",
    "    def conditionalNgram(self, ngram, label):\n",
    "        alpha = self.alpha\n",
    "        if label == 1:\n",
    "            return ((self.trainPositive.get(ngram,0)+alpha) /\n",
    "                    float(self.posGramCount+alpha*self.unique))\n",
    "        else:\n",
    "            return ((self.trainNegative.get(ngram,0)+alpha) /\n",
    "                    float(self.negGramCount+alpha*self.unique))\n",
    "            \n",
    "    def evaluate_test_data(self):\n",
    "        print '\\nTesting Naive Bayesian Classifier'\n",
    "        results = []\n",
    "        for test in self.test_data:\n",
    "            label = test[0]\n",
    "            text = test[1]\n",
    "            print '\\n- Class (c): %d\\n- grams:'%(label)\n",
    "            print text\n",
    "            ruling = self.classify(text)\n",
    "            print '- Classified as (%d)\\n-labeled as (%d)'%(ruling, label)\n",
    "            if ruling == label:\n",
    "                print 'correct!'\n",
    "                results.append(1) \n",
    "            else:\n",
    "                print 'wrong.'\n",
    "                results.append(0) \n",
    "        print(\"Evaluated {} test cases. {:.2f}% Accuracy\".format(len(results), 100.0*sum(results)/float(len(results))))\n",
    "        return sum(results)/float(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1-gram data:\n",
      "[[1, [u'', u'chinese', u'beijing', u'chinese']], [0, [u'', u'japan', u'sapporo']], [0, [u'', u'taiwan', u'taiwan', u'sapporo']], [1, [u'', u'macao', u'taiwan', u'shanghai']], [1, [u'', u'chinese', u'chinese', u'chinese', u'tokyo', u'japan']], [0, [u'', u'tokyo', u'japan', u'chinese']], [1, [u'', u'taipei', u'taiwan']]]\n",
      "\n",
      "Testing 1-gram data:\n",
      "[[1, [u'', u'chinese', u'macao']], [0, [u'', u'sapporo', u'osaka', u'taiwan']], [1, [u'', u'chinese', u'chinese', u'shanghai']]]\n",
      "\n",
      "Vocabulary in training data\n",
      "set([u'', u'beijing', u'chinese', u'tokyo', u'shanghai', u'japan', u'taiwan', u'macao', u'sapporo', u'taipei'])\n",
      "\n",
      "Unique 1-grams in training data\n",
      "N = 10\n",
      "\n",
      "Training Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'chinese', u'beijing', u'chinese']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u'', u'japan', u'sapporo']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u'', u'taiwan', u'taiwan', u'sapporo']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'macao', u'taiwan', u'shanghai']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'chinese', u'chinese', u'chinese', u'tokyo', u'japan']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u'', u'tokyo', u'japan', u'chinese']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'taipei', u'taiwan']\n",
      "\n",
      "Positive Training Vocabulary\n",
      "{u'': 4, u'beijing': 1, u'chinese': 5, u'tokyo': 1, u'shanghai': 1, u'japan': 1, u'taiwan': 2, u'macao': 1, u'taipei': 1}\n",
      "\n",
      "Negative Training Vocabulary\n",
      "{u'': 3, u'chinese': 1, u'tokyo': 1, u'japan': 2, u'taiwan': 2, u'sapporo': 2}\n",
      "\n",
      "Prior(1) = 0.571429\n",
      "\n",
      "Prior(0) = 0.428571\n",
      "\n",
      "Testing Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'chinese', u'macao']\n",
      "P(t|1) = 0.001742\n",
      "P(t|0) = 0.000370\n",
      "- Classified as (1)\n",
      "-labeled as (1)\n",
      "correct!\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u'', u'sapporo', u'osaka', u'taiwan']\n",
      "P(t|1) = 0.000016\n",
      "P(t|0) = 0.000079\n",
      "- Classified as (0)\n",
      "-labeled as (0)\n",
      "correct!\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u'', u'chinese', u'chinese', u'shanghai']\n",
      "P(t|1) = 0.000387\n",
      "P(t|0) = 0.000035\n",
      "- Classified as (1)\n",
      "-labeled as (1)\n",
      "correct!\n",
      "Evaluated 3 test cases. 100.00% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bayes = ngrams_bayes(data,1)\n",
    "unigram_bayes.train()\n",
    "unigram_bayes.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2-gram data:\n",
      "[[1, [u' chinese', u'chinese chinese', u'chinese shanghai']], [1, [u' chinese', u'chinese macao']], [0, [u' taiwan', u'taiwan taiwan', u'taiwan sapporo']], [0, [u' japan', u'japan sapporo']], [1, [u' macao', u'macao taiwan', u'taiwan shanghai']], [1, [u' chinese', u'chinese chinese', u'chinese chinese', u'chinese tokyo', u'tokyo japan']], [1, [u' taipei', u'taipei taiwan']]]\n",
      "\n",
      "Testing 2-gram data:\n",
      "[[0, [u' tokyo', u'tokyo japan', u'japan chinese']], [0, [u' sapporo', u'sapporo osaka', u'osaka taiwan']], [1, [u' chinese', u'chinese beijing', u'beijing chinese']]]\n",
      "\n",
      "Vocabulary in training data\n",
      "set([u'japan sapporo', u'chinese macao', u'taiwan shanghai', u' chinese', u'chinese chinese', u' taipei', u'taiwan sapporo', u'chinese tokyo', u'macao taiwan', u'tokyo japan', u'taiwan taiwan', u'taipei taiwan', u'chinese shanghai', u' japan', u' taiwan', u' macao'])\n",
      "\n",
      "Unique 2-grams in training data\n",
      "N = 16\n",
      "\n",
      "Training Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese', u'chinese chinese', u'chinese shanghai']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese', u'chinese macao']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' taiwan', u'taiwan taiwan', u'taiwan sapporo']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' japan', u'japan sapporo']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' macao', u'macao taiwan', u'taiwan shanghai']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese', u'chinese chinese', u'chinese chinese', u'chinese tokyo', u'tokyo japan']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' taipei', u'taipei taiwan']\n",
      "\n",
      "Positive Training Vocabulary\n",
      "{u'macao taiwan': 1, u'chinese macao': 1, u'taiwan shanghai': 1, u' chinese': 3, u'chinese chinese': 3, u'chinese tokyo': 1, u'tokyo japan': 1, u'taipei taiwan': 1, u'chinese shanghai': 1, u' taipei': 1, u' macao': 1}\n",
      "\n",
      "Negative Training Vocabulary\n",
      "{u'taiwan sapporo': 1, u' japan': 1, u' taiwan': 1, u'taiwan taiwan': 1, u'japan sapporo': 1}\n",
      "\n",
      "Prior(1) = 0.714286\n",
      "\n",
      "Prior(0) = 0.285714\n",
      "\n",
      "Testing Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' tokyo', u'tokyo japan', u'japan chinese']\n",
      "P(t|1) = 0.000048\n",
      "P(t|0) = 0.000031\n",
      "- Classified as (1)\n",
      "-labeled as (0)\n",
      "wrong.\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' sapporo', u'sapporo osaka', u'osaka taiwan']\n",
      "P(t|1) = 0.000024\n",
      "P(t|0) = 0.000031\n",
      "- Classified as (0)\n",
      "-labeled as (0)\n",
      "correct!\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese', u'chinese beijing', u'beijing chinese']\n",
      "P(t|1) = 0.000096\n",
      "P(t|0) = 0.000031\n",
      "- Classified as (1)\n",
      "-labeled as (1)\n",
      "correct!\n",
      "Evaluated 3 test cases. 66.67% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_sms= ngrams_bayes(data,2) \n",
    "bigram_sms.train()\n",
    "bigram_sms.evaluate_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3-gram data:\n",
      "[[0, [u' tokyo japan', u'tokyo japan chinese']], [1, [u' chinese chinese', u'chinese chinese shanghai']], [1, [u' chinese chinese', u'chinese chinese chinese', u'chinese chinese tokyo', u'chinese tokyo japan']], [1, [u' taipei taiwan']], [0, [u' sapporo osaka', u'sapporo osaka taiwan']], [0, [u' japan sapporo']], [0, [u' taiwan taiwan', u'taiwan taiwan sapporo']]]\n",
      "\n",
      "Testing 3-gram data:\n",
      "[[1, [u' chinese beijing', u'chinese beijing chinese']], [1, [u' chinese macao']], [1, [u' macao taiwan', u'macao taiwan shanghai']]]\n",
      "\n",
      "Vocabulary in training data\n",
      "set([u' tokyo japan', u'chinese chinese shanghai', u'tokyo japan chinese', u' taipei taiwan', u'chinese chinese tokyo', u'sapporo osaka taiwan', u' japan sapporo', u' sapporo osaka', u' chinese chinese', u' taiwan taiwan', u'chinese tokyo japan', u'chinese chinese chinese', u'taiwan taiwan sapporo'])\n",
      "\n",
      "Unique 3-grams in training data\n",
      "N = 13\n",
      "\n",
      "Training Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' tokyo japan', u'tokyo japan chinese']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese chinese', u'chinese chinese shanghai']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese chinese', u'chinese chinese chinese', u'chinese chinese tokyo', u'chinese tokyo japan']\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' taipei taiwan']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' sapporo osaka', u'sapporo osaka taiwan']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' japan sapporo']\n",
      "\n",
      "- Class (c): 0\n",
      "- grams:\n",
      "[u' taiwan taiwan', u'taiwan taiwan sapporo']\n",
      "\n",
      "Positive Training Vocabulary\n",
      "{u'chinese chinese shanghai': 1, u' taipei taiwan': 1, u'chinese chinese tokyo': 1, u' chinese chinese': 2, u'chinese tokyo japan': 1, u'chinese chinese chinese': 1}\n",
      "\n",
      "Negative Training Vocabulary\n",
      "{u' tokyo japan': 1, u'sapporo osaka taiwan': 1, u'tokyo japan chinese': 1, u'taiwan taiwan sapporo': 1, u' sapporo osaka': 1, u' taiwan taiwan': 1, u' japan sapporo': 1}\n",
      "\n",
      "Prior(1) = 0.428571\n",
      "\n",
      "Prior(0) = 0.571429\n",
      "\n",
      "Testing Naive Bayesian Classifier\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese beijing', u'chinese beijing chinese']\n",
      "P(t|1) = 0.001071\n",
      "P(t|0) = 0.001429\n",
      "- Classified as (0)\n",
      "-labeled as (1)\n",
      "wrong.\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' chinese macao']\n",
      "P(t|1) = 0.021429\n",
      "P(t|0) = 0.028571\n",
      "- Classified as (0)\n",
      "-labeled as (1)\n",
      "wrong.\n",
      "\n",
      "- Class (c): 1\n",
      "- grams:\n",
      "[u' macao taiwan', u'macao taiwan shanghai']\n",
      "P(t|1) = 0.001071\n",
      "P(t|0) = 0.001429\n",
      "- Classified as (0)\n",
      "-labeled as (1)\n",
      "wrong.\n",
      "Evaluated 3 test cases. 0.00% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sms = ngrams_bayes(data,3) \n",
    "trigram_sms.train()\n",
    "trigram_sms.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our bayesian classifier performs well with unigrams, ok with bigrams, and is basically guessing randomly when it comes to trigrams. Increasing the size of your grams does not help this classifier classify this dataset. This is likely due to the data being made of up of short messages with highly specific and colloquial words. Nearly none of the trigrams will occur more than once in this dataset. I imagine that larger ngrams used in a baysian classifer would work well with something like product reviews which are longer than text messages and use less colloquial language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classifier with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Preprocessing of the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class                                   Document\n",
      "0      1                [chinese, beijing, chinese]\n",
      "1      1               [chinese, chinese, shanghai]\n",
      "2      1                           [chinese, macao]\n",
      "3      0                    [tokyo, japan, chinese]\n",
      "4      1                           [taipei, taiwan]\n",
      "5      1                  [macao, taiwan, shanghai]\n",
      "6      0                           [japan, sapporo]\n",
      "7      0                   [sapporo, osaka, taiwan]\n",
      "8      1  [chinese, chinese, chinese, tokyo, japan]\n",
      "9      0                  [taiwan, taiwan, sapporo]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words in every message\n",
    "df['Document'] = df['Document'].apply(nltk.word_tokenize)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class                             Document\n",
      "0      1                 [chine, beij, chine]\n",
      "1      1             [chine, chine, shanghai]\n",
      "2      1                       [chine, macao]\n",
      "3      0                [tokyo, japan, chine]\n",
      "4      1                     [taipei, taiwan]\n",
      "5      1            [macao, taiwan, shanghai]\n",
      "6      0                     [japan, sapporo]\n",
      "7      0             [sapporo, osaka, taiwan]\n",
      "8      1  [chine, chine, chine, tokyo, japan]\n",
      "9      0            [taiwan, taiwan, sapporo]\n"
     ]
    }
   ],
   "source": [
    "# Use Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "df['Document'] = df['Document'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (2, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 9)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 7)\t1\n",
      "  (5, 8)\t1\n",
      "  (5, 3)\t1\n",
      "  (5, 6)\t1\n",
      "  (6, 5)\t1\n",
      "  (6, 2)\t1\n",
      "  (7, 4)\t1\n",
      "  (7, 5)\t1\n",
      "  (7, 8)\t1\n",
      "  (8, 2)\t1\n",
      "  (8, 9)\t1\n",
      "  (8, 1)\t3\n",
      "  (9, 5)\t1\n",
      "  (9, 8)\t2\n",
      "   Class                          Document\n",
      "0      1                chines beij chines\n",
      "1      1            chines chines shanghai\n",
      "2      1                      chines macao\n",
      "3      0                tokyo japan chines\n",
      "4      1                     taipei taiwan\n",
      "5      1             macao taiwan shanghai\n",
      "6      0                     japan sapporo\n",
      "7      0              sapporo osaka taiwan\n",
      "8      1  chines chines chines tokyo japan\n",
      "9      0             taiwan taiwan sapporo\n"
     ]
    }
   ],
   "source": [
    "# This converts the list of words into space-separated strings\n",
    "df['Document'] = df['Document'].apply(lambda x: ' '.join(x))\n",
    "count_vect = CountVectorizer()  \n",
    "counts = count_vect.fit_transform(df['Document'])\n",
    "print counts\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.764949063238371\n",
      "  (0, 0)\t0.6440907782686681\n",
      "  (1, 1)\t0.8131579832939391\n",
      "  (1, 6)\t0.5820430346678277\n",
      "  (2, 1)\t0.5726575030814076\n",
      "  (2, 3)\t0.8197947207469487\n",
      "  (3, 1)\t0.4653431056969696\n",
      "  (3, 9)\t0.6661675073383854\n",
      "  (3, 2)\t0.5828178498869621\n",
      "  (4, 7)\t0.8341378713086336\n",
      "  (4, 8)\t0.5515559913995145\n",
      "  (5, 6)\t0.61957540186839\n",
      "  (5, 3)\t0.61957540186839\n",
      "  (5, 8)\t0.48192597232276857\n",
      "  (6, 2)\t0.7071067811865475\n",
      "  (6, 5)\t0.7071067811865475\n",
      "  (7, 8)\t0.4686906257315891\n",
      "  (7, 5)\t0.5271685592911145\n",
      "  (7, 4)\t0.708817612257386\n",
      "  (8, 1)\t0.8445513958589379\n",
      "  (8, 9)\t0.4030092286105375\n",
      "  (8, 2)\t0.352585452631641\n",
      "  (9, 8)\t0.8716186013638371\n",
      "  (9, 5)\t0.4901846731146827\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)  \n",
    "print counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "  (0, 8)\t0.48192597232276857\n",
      "  (0, 3)\t0.61957540186839\n",
      "  (0, 6)\t0.61957540186839\n",
      "  (1, 0)\t0.6440907782686681\n",
      "  (1, 1)\t0.764949063238371\n",
      "  (2, 5)\t0.4901846731146827\n",
      "  (2, 8)\t0.8716186013638371\n",
      "  (3, 6)\t0.5820430346678277\n",
      "  (3, 1)\t0.8131579832939391\n",
      "  (4, 8)\t0.5515559913995145\n",
      "  (4, 7)\t0.8341378713086336\n",
      "  (5, 4)\t0.708817612257386\n",
      "  (5, 5)\t0.5271685592911145\n",
      "  (5, 8)\t0.4686906257315891\n",
      "  (6, 5)\t0.7071067811865475\n",
      "  (6, 2)\t0.7071067811865475\n",
      "\n",
      "X_test:\n",
      "  (0, 2)\t0.5828178498869621\n",
      "  (0, 9)\t0.6661675073383854\n",
      "  (0, 1)\t0.4653431056969696\n",
      "  (1, 2)\t0.352585452631641\n",
      "  (1, 9)\t0.4030092286105375\n",
      "  (1, 1)\t0.8445513958589379\n",
      "  (2, 3)\t0.8197947207469487\n",
      "  (2, 1)\t0.5726575030814076\n",
      "\n",
      "y_train:\n",
      "5    1\n",
      "0    1\n",
      "9    0\n",
      "1    1\n",
      "4    1\n",
      "7    0\n",
      "6    0\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "y_test:\n",
      "3    0\n",
      "8    1\n",
      "2    1\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Accuracy: 0.666667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[0 1]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(counts, df['Class'], test_size=0.25, random_state=69)\n",
    "print 'X_train:'\n",
    "print X_train\n",
    "print '\\nX_test:'\n",
    "print X_test\n",
    "print '\\ny_train:'\n",
    "print y_train\n",
    "print '\\ny_test:'\n",
    "print y_test\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "print '\\nAccuracy: %f'%(np.mean(predicted == y_test))\n",
    "print '\\nConfusion Matrix:'\n",
    "print confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training and Evaluating the Multivariate Bernoulli Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "  (0, 8)\t0.48192597232276857\n",
      "  (0, 3)\t0.61957540186839\n",
      "  (0, 6)\t0.61957540186839\n",
      "  (1, 0)\t0.6440907782686681\n",
      "  (1, 1)\t0.764949063238371\n",
      "  (2, 5)\t0.4901846731146827\n",
      "  (2, 8)\t0.8716186013638371\n",
      "  (3, 6)\t0.5820430346678277\n",
      "  (3, 1)\t0.8131579832939391\n",
      "  (4, 8)\t0.5515559913995145\n",
      "  (4, 7)\t0.8341378713086336\n",
      "  (5, 4)\t0.708817612257386\n",
      "  (5, 5)\t0.5271685592911145\n",
      "  (5, 8)\t0.4686906257315891\n",
      "  (6, 5)\t0.7071067811865475\n",
      "  (6, 2)\t0.7071067811865475\n",
      "\n",
      "X_test:\n",
      "  (0, 2)\t0.5828178498869621\n",
      "  (0, 9)\t0.6661675073383854\n",
      "  (0, 1)\t0.4653431056969696\n",
      "  (1, 2)\t0.352585452631641\n",
      "  (1, 9)\t0.4030092286105375\n",
      "  (1, 1)\t0.8445513958589379\n",
      "  (2, 3)\t0.8197947207469487\n",
      "  (2, 1)\t0.5726575030814076\n",
      "\n",
      "y_train:\n",
      "5    1\n",
      "0    1\n",
      "9    0\n",
      "1    1\n",
      "4    1\n",
      "7    0\n",
      "6    0\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "y_test:\n",
      "3    0\n",
      "8    1\n",
      "2    1\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Accuracy: 0.666667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[0 1]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(counts, df['Class'], test_size=0.25, random_state=69)\n",
    "print 'X_train:'\n",
    "print X_train\n",
    "print '\\nX_test:'\n",
    "print X_test\n",
    "print '\\ny_train:'\n",
    "print y_train\n",
    "print '\\ny_test:'\n",
    "print y_test\n",
    "model = BernoulliNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "print '\\nAccuracy: %f'%(np.mean(predicted == y_test))\n",
    "print '\\nConfusion Matrix:'\n",
    "print confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive Bayesian Classification System for lines from Biggie Smalls and 2Pac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biggie_df = pd.read_csv('./biggie_lyrics.csv', usecols=[1], encoding='latin-1', header=None)\n",
    "biggie_df.columns = [\"lyrics\"]\n",
    "biggie_df[\"lyrics\"] = biggie_df[\"lyrics\"].str.replace('[^\\w\\s]','')\n",
    "biggie_df[\"lyrics\"] = biggie_df[\"lyrics\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relax and take notes while i take tokes of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good evenin ladies and gentlemen\\nhows everybo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>who shot ya\\nseperate the weak from the obsole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>when i die fuck it i wanna go to hell\\ncause i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>when the lala hits ya lyrics just splits ya\\nh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lyrics\n",
       "11  relax and take notes while i take tokes of the...\n",
       "12  good evenin ladies and gentlemen\\nhows everybo...\n",
       "13  who shot ya\\nseperate the weak from the obsole...\n",
       "14  when i die fuck it i wanna go to hell\\ncause i...\n",
       "15  when the lala hits ya lyrics just splits ya\\nh..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggie_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pac_df = pd.read_csv('./2pac_lyrics.csv', usecols=[1], encoding='latin-1', header=None)\n",
    "pac_df.columns = [\"lyrics\"]\n",
    "pac_df[\"lyrics\"] = pac_df[\"lyrics\"].str.replace('[^\\w\\s]','')\n",
    "pac_df[\"lyrics\"] = pac_df[\"lyrics\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little something for my godson elijah\\nand a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yo mo bee mayn drop that shit\\nyou know what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rest in peace to my motherfucker biggy smallz\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>makaveli in this killuminati\\nall through your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its just me against the world\\nnothin to lose\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics\n",
       "0  little something for my godson elijah\\nand a l...\n",
       "1  yo mo bee mayn drop that shit\\nyou know what t...\n",
       "2  rest in peace to my motherfucker biggy smallz\\...\n",
       "3  makaveli in this killuminati\\nall through your...\n",
       "4  its just me against the world\\nnothin to lose\\..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fuck all you hoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>get a grip motherfucker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>yeah this album is dedicated to all the teache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>id never amount to nothin to all the people th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>buildings that i was hustlin in front of that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               line\n",
       "0      0                                  fuck all you hoes\n",
       "1      0                            get a grip motherfucker\n",
       "2      0  yeah this album is dedicated to all the teache...\n",
       "3      0  id never amount to nothin to all the people th...\n",
       "4      0  buildings that i was hustlin in front of that ..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggie_lyrics = biggie_df[\"lyrics\"].values\n",
    "biggie_lyrics = [ song.split('\\n') for song in biggie_lyrics]\n",
    "biggie_lyrics = [line for song in biggie_lyrics for line in song]\n",
    "pac_lyrics = pac_df[\"lyrics\"].values\n",
    "pac_lyrics = [ song.split('\\n') for song in pac_lyrics]\n",
    "pac_lyrics = [line for song in pac_lyrics for line in song]\n",
    "\n",
    "rap_lines = [] \n",
    "\n",
    "for line in biggie_lyrics:\n",
    "    if len(line.split()) > 3:\n",
    "        rap_lines.append(np.array([0,str(line)]))\n",
    "        \n",
    "for line in pac_lyrics:\n",
    "    if len(line.split()) > 3:\n",
    "        rap_lines.append(np.array([1,str(line)]))\n",
    "        \n",
    "rap_lines = np.array(rap_lines)\n",
    "rap_lines = pd.DataFrame(rap_lines)\n",
    "rap_lines.columns = [\"label\",\"line\"]\n",
    "rap_lines['label'] = rap_lines['label'].replace(['0','1'],[0,1])\n",
    "rap_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 849)\t0.49087318817854825\n",
      "  (0, 58)\t0.43268940172940906\n",
      "  (0, 2501)\t0.31850678004835703\n",
      "  (0, 1027)\t0.6858402334521511\n",
      "  (1, 877)\t0.41830230655719114\n",
      "  (1, 932)\t0.7601505859197147\n",
      "  (1, 1431)\t0.49718635043095055\n",
      "  (2, 58)\t0.20803213246469884\n",
      "  (2, 2493)\t0.34411189665671293\n",
      "  (2, 2215)\t0.2250169996870463\n",
      "  (2, 53)\t0.4322487816477064\n",
      "  (2, 1131)\t0.2057966169419576\n",
      "  (2, 561)\t0.410044488257387\n",
      "  (2, 2255)\t0.1425118077471302\n",
      "  (2, 2203)\t0.1160197544126776\n",
      "  (2, 2176)\t0.4322487816477064\n",
      "  (2, 2201)\t0.2028127062967269\n",
      "  (2, 2261)\t0.3183733108460118\n",
      "  (2, 1357)\t0.16506204370082317\n",
      "  (3, 58)\t0.20116206656478056\n",
      "  (3, 2255)\t0.2756109781373328\n",
      "  (3, 2203)\t0.22437662185626947\n",
      "  (3, 2201)\t0.19611500704665685\n",
      "  (3, 1089)\t0.3383271897309154\n",
      "  (3, 1467)\t0.24968336690403506\n",
      "  :\t:\n",
      "  (1967, 2203)\t0.15468580785408723\n",
      "  (1967, 1357)\t0.22007248425203085\n",
      "  (1967, 1515)\t0.2704043589421255\n",
      "  (1967, 1239)\t0.34580186212426445\n",
      "  (1967, 2102)\t0.37858120072806645\n",
      "  (1967, 1199)\t0.5094034783786443\n",
      "  (1967, 1893)\t0.5763048915384901\n",
      "  (1968, 2255)\t0.1421446866227497\n",
      "  (1968, 2203)\t0.11572087880816694\n",
      "  (1968, 78)\t0.15708416007245224\n",
      "  (1968, 118)\t0.2635537662293054\n",
      "  (1968, 633)\t0.25213286835978227\n",
      "  (1968, 846)\t0.2553156008029875\n",
      "  (1968, 1511)\t0.4311352763091721\n",
      "  (1968, 1832)\t0.4311352763091721\n",
      "  (1968, 143)\t0.4311352763091721\n",
      "  (1968, 92)\t0.4311352763091721\n",
      "  (1969, 1131)\t0.2319798097665941\n",
      "  (1969, 633)\t0.2849453932277956\n",
      "  (1969, 2211)\t0.24215750156044613\n",
      "  (1969, 1702)\t0.36286342155575707\n",
      "  (1969, 2428)\t0.3516087338941551\n",
      "  (1969, 1313)\t0.4444552497821418\n",
      "  (1969, 2205)\t0.43068062533342516\n",
      "  (1969, 321)\t0.4099102348400397\n",
      "      label                                               line\n",
      "0         0                                   fuck all you hoe\n",
      "1         0                              get a grip motherfuck\n",
      "2         0  yeah thi album is dedic to all the teacher tha...\n",
      "3         0  id never amount to nothin to all the peopl tha...\n",
      "4         0  build that i wa hustlin in front of that call ...\n",
      "5         0  me when i wa just tryin to make some money to ...\n",
      "6         0  and all the nigga in the struggl you know what...\n",
      "7         0                     uhha it all good babi baybe uh\n",
      "8         0                                  it wa all a dream\n",
      "9         0                      i use to read word up magazin\n",
      "10        0           saltnpepa and heavi d up in the limousin\n",
      "11        0                           hangin pictur on my wall\n",
      "12        0     everi saturday rap attack mr magic marley marl\n",
      "13        0                 i let my tape rock til my tape pop\n",
      "14        0      smokin weed and bamboo sippin on privat stock\n",
      "15        0   way back when i had the red and black lumberjack\n",
      "16        0                              with the hat to match\n",
      "17        0                     rememb rappin duke duhha duhha\n",
      "18        0  you never thought that hip hop would take it t...\n",
      "19        0         now im in the limelight caus i rhyme tight\n",
      "20        0      time to get paid blow up like the world trade\n",
      "21        0                born sinner the opposit of a winner\n",
      "22        0         rememb when i use to eat sardin for dinner\n",
      "23        0                   peac to ron g brucey b kid capri\n",
      "24        0                      funkmast flex lovebug starski\n",
      "25        0              im blowin up like you thought i would\n",
      "26        0                call the crib same number same hood\n",
      "27        0      uh and if you dont know now you know nigga uh\n",
      "28        0                     you know veri well who you are\n",
      "29        0       dont let em hold you down reach for the star\n",
      "...     ...                                                ...\n",
      "1940      1                dont let em jack you up back you up\n",
      "1941      1                  crack you up and pimpsmack you up\n",
      "1942      1                  you got ta learn to hold your own\n",
      "1943      1  they get jealou when they see you with your mo...\n",
      "1944      1               but tell the cop they cant touch thi\n",
      "1945      1  i dont trust thi when they tri to rush i bust thi\n",
      "1946      1                          that the sound of my tool\n",
      "1947      1    you say it aint cool my mama didnt rais no fool\n",
      "1948      1    and as long as i stay black i got ta stay strap\n",
      "1949      1                        and i never get to lay back\n",
      "1950      1         caus i alway got to worri bout the payback\n",
      "1951      1                 some buck that i rough up way back\n",
      "1952      1                     come back after all these year\n",
      "1953      1                ratatattattattat that the way it is\n",
      "1954      1     out on bail fresh out of jail california dream\n",
      "1955      1  soon as i step on the scene im hear hoochi scream\n",
      "1956      1                        fiend for money and alcohol\n",
      "1957      1  the life of a westsid player where coward die ...\n",
      "1958      1  onli in cali where we riot not ralli to live a...\n",
      "1959      1      in la we wear chuck not balli yeah that right\n",
      "1960      1  dress in loc and khaki suit and ride is what w...\n",
      "1961      1   floss but have caution we collid with other crew\n",
      "1962      1                            famou becaus we program\n",
      "1963      1  worldwid let them recogn from long beach to ro...\n",
      "1964      1          bump and grind like a slow jam it westsid\n",
      "1965      1        so you know the row wont bow down to no man\n",
      "1966      1  say what you say but give me that bomb beat fr...\n",
      "1967      1                    let me serenad the street of la\n",
      "1968      1  from oakland to sactown the bay area and back ...\n",
      "1969      1             cali is where they put their mack down\n",
      "\n",
      "[1970 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words in every message\n",
    "rap_lines['line'] = rap_lines['line'].apply(nltk.word_tokenize)\n",
    "# Use Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "rap_lines['line'] = rap_lines['line'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# This converts the list of words into space-separated strings\n",
    "rap_lines['line'] = rap_lines['line'].apply(lambda x: ' '.join(x))\n",
    "count_vect = CountVectorizer()  \n",
    "counts = count_vect.fit_transform(rap_lines['line'])\n",
    "#\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)  \n",
    "print counts\n",
    "print rap_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "  (0, 1873)\t0.4823154015554264\n",
      "  (0, 2016)\t0.4823154015554264\n",
      "  (0, 1629)\t0.37058194665767275\n",
      "  (0, 1648)\t0.37448705119244685\n",
      "  (0, 47)\t0.2632289821958594\n",
      "  (0, 187)\t0.3384396815640666\n",
      "  (0, 1490)\t0.270815917221267\n",
      "  (1, 2498)\t0.5117929290019098\n",
      "  (1, 1035)\t0.40652117615694483\n",
      "  (1, 1542)\t0.3282981294771844\n",
      "  (1, 1588)\t0.4475722319501924\n",
      "  (1, 1255)\t0.29352061990807393\n",
      "  (1, 1132)\t0.23386804543897377\n",
      "  (1, 1449)\t0.21495478855940697\n",
      "  (1, 58)\t0.2786642933984189\n",
      "  (2, 155)\t0.5679236022476969\n",
      "  (2, 900)\t0.24835987179690938\n",
      "  (2, 1976)\t0.3273955356380066\n",
      "  (2, 1460)\t0.29318591576310293\n",
      "  (2, 2323)\t0.2957671450408179\n",
      "  (2, 866)\t0.31463692798979265\n",
      "  (2, 2423)\t0.2367709073460962\n",
      "  (2, 2426)\t0.2070408450116228\n",
      "  (2, 2255)\t0.2753541827873831\n",
      "  (2, 1131)\t0.19881496198199047\n",
      "  :\t:\n",
      "  (1474, 2431)\t0.3054807936578063\n",
      "  (1474, 2219)\t0.3102263656060515\n",
      "  (1474, 2070)\t0.34269829448833733\n",
      "  (1474, 595)\t0.2575839291125625\n",
      "  (1474, 2507)\t0.2027509499827377\n",
      "  (1474, 2229)\t0.3274303467273544\n",
      "  (1474, 910)\t0.34269829448833733\n",
      "  (1474, 2255)\t0.14811043943583094\n",
      "  (1475, 1388)\t0.3974350128661169\n",
      "  (1475, 1117)\t0.3974350128661169\n",
      "  (1475, 1767)\t0.36750179314157966\n",
      "  (1475, 779)\t0.40310279835030244\n",
      "  (1475, 1616)\t0.40931115491561065\n",
      "  (1475, 487)\t0.3922211556400549\n",
      "  (1475, 1449)\t0.19087546782001524\n",
      "  (1475, 2255)\t0.16951367617575044\n",
      "  (1476, 1317)\t0.45265088015790717\n",
      "  (1476, 2020)\t0.45265088015790717\n",
      "  (1476, 1388)\t0.349898292088227\n",
      "  (1476, 1117)\t0.349898292088227\n",
      "  (1476, 900)\t0.26921557711533584\n",
      "  (1476, 562)\t0.37315062586716663\n",
      "  (1476, 1239)\t0.27160539420883517\n",
      "  (1476, 1515)\t0.21238544539668033\n",
      "  (1476, 2203)\t0.12149587501534005\n",
      "\n",
      "X_test:\n",
      "  (0, 1240)\t0.4559616304957513\n",
      "  (0, 1376)\t0.4559616304957513\n",
      "  (0, 1371)\t0.4559616304957513\n",
      "  (0, 2403)\t0.41592076549335977\n",
      "  (0, 1200)\t0.41592076549335977\n",
      "  (0, 1357)\t0.1741172254827283\n",
      "  (1, 1821)\t0.5097495374155772\n",
      "  (1, 1872)\t0.2624910415365172\n",
      "  (1, 914)\t0.23917634382372452\n",
      "  (1, 2057)\t0.4505743249344905\n",
      "  (1, 2003)\t0.36104568459474673\n",
      "  (1, 2438)\t0.33704964094040557\n",
      "  (1, 2423)\t0.2890284325633704\n",
      "  (1, 2375)\t0.2890284325633704\n",
      "  (2, 1110)\t0.4156414445714651\n",
      "  (2, 603)\t0.4156414445714651\n",
      "  (2, 294)\t0.7347818862262353\n",
      "  (2, 1255)\t0.21070400011480386\n",
      "  (2, 117)\t0.26492222088478706\n",
      "  (3, 2116)\t0.4134248589505195\n",
      "  (3, 966)\t0.467721126002392\n",
      "  (3, 486)\t0.3524131201078532\n",
      "  (3, 2229)\t0.34090809616392304\n",
      "  (3, 1725)\t0.4436946450785613\n",
      "  (3, 1132)\t0.18891793578937982\n",
      "  :\t:\n",
      "  (489, 1301)\t0.5015232000936662\n",
      "  (489, 194)\t0.336150769019593\n",
      "  (489, 1747)\t0.45748128646009134\n",
      "  (489, 951)\t0.336150769019593\n",
      "  (489, 118)\t0.30658203004773993\n",
      "  (489, 2400)\t0.3253555455593667\n",
      "  (489, 78)\t0.18273000371945194\n",
      "  (489, 2426)\t0.24865800694856544\n",
      "  (489, 2203)\t0.13461367845925096\n",
      "  (490, 1088)\t0.47950149071421866\n",
      "  (490, 938)\t0.47950149071421866\n",
      "  (490, 77)\t0.42944941566169187\n",
      "  (490, 145)\t0.27994081709149204\n",
      "  (490, 380)\t0.43480074347237735\n",
      "  (490, 1132)\t0.23021474656770097\n",
      "  (490, 2255)\t0.1879161064366237\n",
      "  (491, 2457)\t0.6283510926722093\n",
      "  (491, 1835)\t0.4579865283320731\n",
      "  (491, 1913)\t0.4076332112273358\n",
      "  (491, 1093)\t0.34880697434005586\n",
      "  (491, 877)\t0.32801235336523804\n",
      "  (492, 1067)\t0.6783061557069535\n",
      "  (492, 1282)\t0.5051533240466801\n",
      "  (492, 18)\t0.4578845077260314\n",
      "  (492, 1132)\t0.27397564840535327\n",
      "\n",
      "y_train:\n",
      "830     0\n",
      "993     1\n",
      "577     0\n",
      "136     0\n",
      "673     0\n",
      "1624    1\n",
      "549     0\n",
      "571     0\n",
      "1938    1\n",
      "966     1\n",
      "268     0\n",
      "1410    1\n",
      "1932    1\n",
      "151     0\n",
      "1718    1\n",
      "328     0\n",
      "1372    1\n",
      "118     0\n",
      "1248    1\n",
      "951     1\n",
      "557     0\n",
      "330     0\n",
      "1835    1\n",
      "49      0\n",
      "242     0\n",
      "416     0\n",
      "1130    1\n",
      "1071    1\n",
      "46      0\n",
      "1006    1\n",
      "       ..\n",
      "687     0\n",
      "760     0\n",
      "920     1\n",
      "1575    1\n",
      "995     1\n",
      "258     0\n",
      "1560    1\n",
      "167     0\n",
      "1360    1\n",
      "1283    1\n",
      "1589    1\n",
      "843     0\n",
      "1780    1\n",
      "106     0\n",
      "933     1\n",
      "1420    1\n",
      "88      0\n",
      "1376    1\n",
      "1830    1\n",
      "1249    1\n",
      "1208    1\n",
      "1033    1\n",
      "278     0\n",
      "1585    1\n",
      "404     0\n",
      "439     0\n",
      "1626    1\n",
      "619     0\n",
      "1227    1\n",
      "1078    1\n",
      "Name: label, Length: 1477, dtype: int64\n",
      "\n",
      "y_test:\n",
      "1725    1\n",
      "176     0\n",
      "750     0\n",
      "39      0\n",
      "421     0\n",
      "558     0\n",
      "1168    1\n",
      "264     0\n",
      "876     1\n",
      "908     1\n",
      "1581    1\n",
      "1288    1\n",
      "602     0\n",
      "1452    1\n",
      "354     0\n",
      "11      0\n",
      "929     1\n",
      "741     0\n",
      "826     0\n",
      "855     0\n",
      "1473    1\n",
      "1571    1\n",
      "984     1\n",
      "1697    1\n",
      "1136    1\n",
      "947     1\n",
      "642     0\n",
      "1921    1\n",
      "800     0\n",
      "1917    1\n",
      "       ..\n",
      "1271    1\n",
      "386     0\n",
      "721     0\n",
      "977     1\n",
      "991     1\n",
      "799     0\n",
      "477     0\n",
      "945     1\n",
      "733     0\n",
      "360     0\n",
      "1060    1\n",
      "696     0\n",
      "1745    1\n",
      "490     0\n",
      "158     0\n",
      "1144    1\n",
      "1424    1\n",
      "1484    1\n",
      "1920    1\n",
      "43      0\n",
      "1395    1\n",
      "333     0\n",
      "1561    1\n",
      "523     0\n",
      "808     0\n",
      "1492    1\n",
      "15      0\n",
      "584     0\n",
      "350     0\n",
      "336     0\n",
      "Name: label, Length: 493, dtype: int64\n",
      "\n",
      "Accuracy: 0.691684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 91 136]\n",
      " [ 16 250]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(counts, rap_lines['label'], test_size=0.25, random_state=69)\n",
    "print 'X_train:'\n",
    "print X_train\n",
    "print '\\nX_test:'\n",
    "print X_test\n",
    "print '\\ny_train:'\n",
    "print y_train\n",
    "print '\\ny_test:'\n",
    "print y_test\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "print '\\nAccuracy: %f'%(np.mean(predicted == y_test))\n",
    "print '\\nConfusion Matrix:'\n",
    "print confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
